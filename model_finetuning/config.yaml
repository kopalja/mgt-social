
model:

trainer:
  learning_rate: 2e-4
  weight_decay: 0.01
  adam_epsilon: 1e-8
  warmup_steps: 100
  epoch: 1
  batch_size:
    aya: 1
    default: 16
  gradient_accumulation_steps: 8
  fp_16: false
  model_save_period: 2